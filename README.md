Deep neural networks (DNNs) have demonstrated impressive success over the past couple of years. However, even with their outstanding performances in a number of applications, little is known about their internal mechanisms and theoretical basis. Currently, there is a heated debate in the usefulness of information theory, especially information bottleneck theory to explain DNNs, and we join this debate with our own data set and DNN. We validate the algorithm proposed in the mainstream literature and find that, the mutual information across layers of a neural network indeed increases, but the information compression process (the mutual information first increase and then decrease) cannot be observed with any kind of activation functions. However, we argue that the current mainstream literature has a high potential to over-fit the neural network with too many epochs (> 10000). We introduce regularization in the training process and indeed observe the information compression process.
